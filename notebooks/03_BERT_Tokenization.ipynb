{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52762967-b780-4384-8876-3963380ce070",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aayma\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoTokenizer # Requires the 'transformers' library\n",
    "import os\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52b1d9e2-49ad-449d-9444-b4a4ebdb3bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. USER SETTINGS ---\n",
    "# The model tokenizer we will use (a general-purpose, robust model)\n",
    "MODEL_NAME = 'bert-base-uncased'\n",
    "# BERT models usually have a max sequence length of 512\n",
    "MAX_LENGTH = 512 \n",
    "\n",
    "# Define paths to the processed data\n",
    "PROCESSED_FOLDER = '../processed_data1/'\n",
    "TRAIN_PATH = os.path.join(PROCESSED_FOLDER, 'train_data_final.csv')\n",
    "VAL_PATH = os.path.join(PROCESSED_FOLDER, 'val_data_final.csv')\n",
    "TEST_PATH = os.path.join(PROCESSED_FOLDER, 'test_data_final.csv')\n",
    "\n",
    "# Load the data\n",
    "train_df = pd.read_csv(TRAIN_PATH)\n",
    "val_df = pd.read_csv(VAL_PATH)\n",
    "test_df = pd.read_csv(TEST_PATH)\n",
    "\n",
    "# Define text and label columns\n",
    "TEXT_COL = 'text'\n",
    "TITLE_COL = 'title'\n",
    "LABEL_COL = 'label'\n",
    "CATEGORY_ENCODED_COL = 'category_encoded'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "501862c2-b824-4a71-8591-c9de5df459ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer for bert-base-uncased loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# --- 2. Initialize BERT Tokenizer ---\n",
    "# We download the specific tokenizer for the BERT model\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "    print(f\"Tokenizer for {MODEL_NAME} loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading tokenizer: {e}. You may need to run: pip install transformers\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94b36801-7b94-4d8f-95a6-3b5dd501f0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_data(df, tokenizer, max_len):\n",
    "   \n",
    "    encodings = tokenizer(\n",
    "        df[TITLE_COL].astype(str).tolist(), \n",
    "        df[TEXT_COL].astype(str).tolist(), \n",
    "        truncation=True,         \n",
    "        padding='max_length',    \n",
    "        max_length=max_len,\n",
    "        return_tensors='pt'      # Return PyTorch tensors\n",
    "    )\n",
    "    \n",
    "    # add labels and category labels\n",
    "    encodings['labels'] = torch.tensor(df[LABEL_COL].tolist(), dtype=torch.long)\n",
    "    encodings['category_labels'] = torch.tensor(df[CATEGORY_ENCODED_COL].tolist(), dtype=torch.long)\n",
    "    \n",
    "    return encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c94145c1-93a1-4e5e-8719-7eeb98f3d3bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting Tokenization...\n",
      "Tokenization Complete.\n",
      "Training Input ID Shape: torch.Size([13000, 512])\n",
      "Validation Input ID Shape: torch.Size([3000, 512])\n",
      "Testing Input ID Shape: torch.Size([4000, 512])\n"
     ]
    }
   ],
   "source": [
    "#--- 4. Apply Tokenization to all Splits ---\n",
    "print(\"\\nStarting Tokenization...\")\n",
    "train_encodings = tokenize_data(train_df, tokenizer, MAX_LENGTH)\n",
    "val_encodings = tokenize_data(val_df, tokenizer, MAX_LENGTH)\n",
    "test_encodings = tokenize_data(test_df, tokenizer, MAX_LENGTH)\n",
    "\n",
    "print(\"Tokenization Complete.\")\n",
    "print(f\"Training Input ID Shape: {train_encodings['input_ids'].shape}\")\n",
    "print(f\"Validation Input ID Shape: {val_encodings['input_ids'].shape}\")\n",
    "print(f\"Testing Input ID Shape: {test_encodings['input_ids'].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de723a98-7dec-4602-bb7c-aa697ee6209b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- SUCCESS ---\n",
      "Tokenized data (PyTorch Tensors) saved to 'tokenized_data/' folder.\n"
     ]
    }
   ],
   "source": [
    "# --- 5. Save the Tokenized Data (Optional, but good practice) ---\n",
    "# Saving tokenized data as a single dictionary file for easy loading in the next step\n",
    "import torch\n",
    "\n",
    "# Create a folder for the final tokenized data\n",
    "tokenized_folder = '../tokenized_data/'\n",
    "if not os.path.exists(tokenized_folder):\n",
    "    os.makedirs(tokenized_folder)\n",
    "\n",
    "torch.save(train_encodings, os.path.join(tokenized_folder, 'train_encodings.pt'))\n",
    "torch.save(val_encodings, os.path.join(tokenized_folder, 'val_encodings.pt'))\n",
    "torch.save(test_encodings, os.path.join(tokenized_folder, 'test_encodings.pt'))\n",
    "\n",
    "print(\"\\n--- SUCCESS ---\")\n",
    "print(\"Tokenized data (PyTorch Tensors) saved to 'tokenized_data/' folder.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dce20e1-ec08-4ee1-b180-57743535f2d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e90114-92d4-4046-84ee-0fb9d65416d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
